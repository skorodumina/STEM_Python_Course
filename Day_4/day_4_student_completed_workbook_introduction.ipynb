{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all packages\n",
    "import scipy,corner,emcee,nestle,warnings\n",
    "import pandas\n",
    "import scipy.optimize\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in some data we created for this example (.dat is a generic filename, it's just a text file)\n",
    "example_data_1D = pandas.read_csv('1D_intro_examples.dat',sep=' ',header=0)#this file is separated by spaces and its first line contains the names of the columns (header) \n",
    "print(example_data_1D.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's plot the data, with error bars, that we read from file (See Day 2)\n",
    "plt.errorbar(example_data_1D['x'], #x,y,and error are the column names\n",
    "             example_data_1D['y'], \n",
    "             yerr=example_data_1D['error'],#yerr denotes an error in the y-direction for plotting\n",
    "             fmt='.') #fmt is \"format\", saying that I want data marked by \"points\"\n",
    "plt.xlabel('x') #set the x-axis label \n",
    "plt.ylabel('y') #set the y-axis label\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data were generated with a simple quadratic equation:\n",
    "#ax^2+bx+c. The true model values are:\n",
    "a_true=2\n",
    "b_true=1.3\n",
    "c_true=6\n",
    "def my_model(x,a,b,c): #We define the model described above\n",
    "    return(a*x**2+b*x+c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Fitting: Chi-Square Minimization**\n",
    "==============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try and fit the data with a chi-square minimization\n",
    "def chisq_likelihood(theta, args):\n",
    "    #This function accepts an argument \"theta\", which is \n",
    "    #a list of model parameters a, b and c. It then calculates\n",
    "    #a chi-square statistic that it returns, which compares\n",
    "    #the observations, errors, and model provided in args.\n",
    "    \n",
    "    x, y, yerr,mod = args #args is a list, so this is the same as x=args[0],y=args[1],yerr=args[2]. x,y, and yerr are numpy arrays, mod is a function.\n",
    "    a,b,c = theta #theta is also a list, so it follows the same as args above\n",
    "    model_observations = mod(x,a,b,c) #mod (a model) is the 4th element of args, and it accepts x values, and the three model parameters a,b,c. Now model_observations contains the model values at every point in x (and is a numpy array)\n",
    "    inv_sigma2 = 1./yerr**2 #The chi-square statistic contains an inverse-square error, which we calculate here\n",
    "    chisquare = np.sum((y-model_observations)**2*inv_sigma2 )#calculate the chi-square statistic. \n",
    "    return chisquare\n",
    "\n",
    "#Use scipy.optimize.minimize to minimize the chi-square, which\n",
    "#is the same as maximizing the likelihood.\n",
    "result = scipy.optimize.minimize(chisq_likelihood, #the first argument is the function to minimize, which must accept a list of parameters as its first argument, and any other necessary agruments as its second.\n",
    "                                 x0=[1,1,1], #x0 is a first guess for each of your parameters (depends on your model/data)\n",
    "                                 bounds=[(-100,100),(-100,100),(-100,100)], #optional bounds for each of your parameters\n",
    "                                 args=[example_data_1D['x'],#the args passed to chisq_likelihood above\n",
    "                                       example_data_1D['y'],\n",
    "                                       example_data_1D['error'],\n",
    "                                       my_model])#my_model is defined in a previous cell\n",
    "\n",
    "print(result)#scipy.optimize.minimize returns a dictionary which I've called result. It has other information, the \"x\" element has the best fit values\n",
    "a_ml,b_ml,c_ml = result[\"x\"] #get our best fit values from result\n",
    "\n",
    "#set up plotting the model over the data\n",
    "plt.errorbar(example_data_1D['x'],\n",
    "             example_data_1D['y'],\n",
    "             yerr=example_data_1D['error'],\n",
    "             fmt='.',\n",
    "             label='Data')\n",
    "\n",
    "plt.plot(example_data_1D['x'],\n",
    "         my_model(example_data_1D['x'],a_ml,b_ml,c_ml),\n",
    "         'g--',#make the line green and dashed\n",
    "         label='$\\chi^2$ Fit')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "print('Chi-square fit:',a_ml, b_ml,c_ml)\n",
    "print('True:',a_true,b_true,c_true)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's look at a drawback of the chi-square minimization.\n",
    "#This fitting technique depends on finding a minimum in the chi-square.    \n",
    "#What if there is some sort of local minimum in the chi-square?\n",
    "\n",
    "#my_bad_model was used to create new data, read in below.\n",
    "#This model has one of the parameters squared, \n",
    "#so either -b or b will give the same result\n",
    "def my_bad_model(x,a,b,c):\n",
    "    return(a*x**2+b**2*x+c)#b is now squared \n",
    "\n",
    "#These are the true values used to make the data\n",
    "a_true_bad=2\n",
    "b_true_bad=-3.3 #note b_true_bad is negative here\n",
    "c_true_bad=6\n",
    "\n",
    "#Read in some data generated with this model. \n",
    "my_bad_data_1D=pandas.read_csv(\"1D_intro_examples_bad.dat\",header=0,sep=' ')\n",
    "\n",
    "#Plot the data\n",
    "plt.errorbar(my_bad_data_1D['x'],my_bad_data_1D['y'],yerr=my_bad_data_1D['error'],fmt='.')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try and fit the data with this model\n",
    "result_bad = scipy.optimize.minimize(chisq_likelihood, \n",
    "                                     [1,1,1],\n",
    "                                     bounds=[(-100,100),(-100,100),(-100,100)], \n",
    "                                     args=[my_bad_data_1D['x'],\n",
    "                                           my_bad_data_1D['y'],\n",
    "                                           my_bad_data_1D['error'],\n",
    "                                           my_bad_model])\n",
    "\n",
    "a_ml, b_ml,c_ml = result_bad[\"x\"] #get our best fit values\n",
    "\n",
    "#Plot the bad data\n",
    "plt.errorbar(my_bad_data_1D['x'],\n",
    "             my_bad_data_1D['y'],\n",
    "             yerr=my_bad_data_1D['error'],\n",
    "             fmt='.',\n",
    "             label='Data')\n",
    "#Plot our fitting result\n",
    "plt.plot(my_bad_data_1D['x'],\n",
    "         my_bad_model(my_bad_data_1D['x'],a_ml,b_ml,c_ml),\n",
    "         'g--',\n",
    "         label='$\\chi^2$ Fit')#note we can use latex directly in labels for matplotlib\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "print('Chi-square fit:',a_ml, b_ml,c_ml)\n",
    "print('True:',a_true_bad,b_true_bad,c_true_bad)\n",
    "plt.show()\n",
    "#However, in this (very contrived) case, the true value of b is -3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Fitting: Markov Chain Monte Carlo (MCMC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can combine a likelihood function with a prior function.\n",
    "\n",
    "# Create a log-prior function. \n",
    "def lnprior(theta):#accepts the model parameters (theta)\n",
    "    a,b,c = theta #set a,b,c (see above)\n",
    "    \n",
    "    #give the following bounds: a=(0,5),b=(-5,5),c=(0,10)\n",
    "    if 0 < a < 5. and -5 < b < 5 and 0 < c < 10: #we are assuming a \"uniform prior\" on all parameters, which is the same as just giving each parameter bounds.\n",
    "        return 0.0 #if you try parameters inside the bounds, return a probability of 1 (log(1)=0)\n",
    "    return -np.inf #if you try parameters outside the bounds, return 0 (log(0)=-inf)\n",
    "\n",
    "# This is a log-likelihood function, which is commonly used.\n",
    "def lnprob(theta, x, y, yerr,mod): #accepts theta (the model parameters), and the same x,y,yerr, and mod from above\n",
    "    lp = lnprior(theta) #get the probability from the prior function\n",
    "    if not np.isfinite(lp): \n",
    "        return -np.inf #return a probability of negative infinity if the prior is negative infinity\n",
    "\n",
    "    #the chisq_likelihood function returns a chi-square, \n",
    "    #which you want to be as small as possible. We are \n",
    "    #maximizing the likelihood here, so we take\n",
    "    #the negative of the chi-square function.\n",
    "    return lp - chisq_likelihood(theta, [x, y, yerr,mod]) #the total likelihood is the product of the prior and the likelihood (or the sum of the log-prior and log-likelihood)\n",
    "\n",
    "\n",
    "#Set up the MCMC to sample the full parameter space\n",
    "ndim, nwalkers = 3, 100 #number of parameters to fit (3); number of individual \"walkers\" that randomly sample the space. Choose any number, the higher the slower.\n",
    "starting_positions = [result[\"x\"] + 0.1*np.random.randn(ndim) for i in range(nwalkers)] #Start the walkers in a random (small) gaussian near the result from the chi-square fit\n",
    "\n",
    "#Let's just look at where walkers are starting in the a-b parameter space\n",
    "#(Doesn't need to be included in your own code)\n",
    "#Notice we get a random sampling near our previous result to start. \n",
    "plt.scatter([x[0] for x in starting_positions],#get the \"a\" parameter locations\n",
    "            [x[1] for x in starting_positions])#get the \"b\" parameter locations\n",
    "plt.scatter(result['x'][0],result['x'][1],color='r')#the chi-square result\n",
    "plt.xlabel('a')\n",
    "plt.ylabel('b')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up the MCMC sampler\n",
    "sampler = emcee.EnsembleSampler(nwalkers, \n",
    "                                ndim, \n",
    "                                lnprob, #the likelihood function to maximize \n",
    "                                args=(example_data_1D['x'], #the arguments passed to the likelihood functions (other than the model parameters)\n",
    "                                      example_data_1D['y'], \n",
    "                                      example_data_1D['error'],\n",
    "                                      my_model))\n",
    "\n",
    "output = sampler.run_mcmc(starting_positions, 500) #run the MCMC with the starting positions we defined and 500 sampling points per walker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampler.chain contains all of the samples from the MCMC.\n",
    "print(sampler.chain.shape)\n",
    "#It currently holds the samples separately for each walker.\n",
    "#We don't care about what each walker does, so let's flatten it:\n",
    "samples = sampler.chain.reshape((-1, ndim)) #The -1 here means we don't care how many rows it takes, give us the same number of columns as we have parameters\n",
    "print(samples.shape)\n",
    "#So we tried 50000 total model realizations for 3 parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So what did we actually get from this? Let's use another\n",
    "#python package to see the output of the MCMC sampling\n",
    "fig = corner.corner(samples, #samples is defined above\n",
    "                    labels=[\"$a$\", \"$b$\",\"$c$\"],#parameter labels\n",
    "                    quantiles=(0.16,0.50,0.84),#plot some quantiles for reference\n",
    "                    truths=[a_true, b_true,c_true])#show the true values for comparison\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#But how close did we get to the true values?\n",
    "#We can take the 50th percentile of the distributions you see above\n",
    "#As our result. Incidentally we could calculate the uncertainty as \n",
    "#well, perhaps as the differences between the 84th and 50th for\n",
    "#the upper uncertainty, and 50th and 16th for the lower uncertainty\n",
    "a_mcmc, b_mcmc,c_mcmc = np.percentile(samples, 50,axis=0)#axis=0 means we want to calculate percentiles along columns, not rows\n",
    "print('Chi:','a=',a_ml,'b=', b_ml,'c=',c_ml)\n",
    "print('MCMC:','a=',a_mcmc,'b=',b_mcmc,'c=',c_mcmc)\n",
    "print('True:','a=',a_true,'b=', b_true,'c=',c_true)\n",
    "#Definitely closer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's see how the MCMC deals with the \"bad\" data/model above\n",
    "#The set up is exactly the same as before.\n",
    "\n",
    "ndim, nwalkers = 3, 100\n",
    "#I've increased the range around the chi-square result\n",
    "#That I want my walkers to start at because I know \n",
    "#We're far from the right answer. You could always make\n",
    "#The walkers start over a wide range to start with,\n",
    "#look at the posterior, and hone in on the correct area\n",
    "#of parameter space. \n",
    "pos = [result_bad[\"x\"] - 2*np.abs(np.random.randn(ndim)) for i in range(nwalkers)]\n",
    "\n",
    "sampler_bad = emcee.EnsembleSampler(nwalkers, \n",
    "                                    ndim, \n",
    "                                    lnprob, \n",
    "                                    args=(my_bad_data_1D['x'], \n",
    "                                          my_bad_data_1D['y'], \n",
    "                                          my_bad_data_1D['error'],\n",
    "                                          my_bad_model))\n",
    "sampler_bad.run_mcmc(pos, 500)\n",
    "samples_bad = sampler_bad.chain.reshape((-1, ndim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's look at the result\n",
    "a_mcmc_bad, b_mcmc_bad,c_mcmc_bad = np.percentile(samples_bad, 50,axis=0)\n",
    "print('MCMC:','a=',a_mcmc_bad,'b=',b_mcmc_bad,'c=',c_mcmc_bad)\n",
    "print('True:','a=',a_true_bad,'b=', b_true_bad,'c=',c_true_bad)\n",
    "fig = corner.corner(samples_bad, labels=[\"$a$\", \"$b$\",\"$c$\"],\n",
    "                      truths=[a_true_bad, b_true_bad,c_true_bad])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Nested Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An alternative to MCMC is Bayesian Nested Sampling. It is\n",
    "#extremely similar but randomly samples the full parameter\n",
    "#space instead of sending out individual walkers. MCMC\n",
    "#is often better for a many-dimensional model.\n",
    "\n",
    "# Define a likelihood function. Let's use a chi-square again.\n",
    "# The package doing the fitting needs a likelihood function\n",
    "# that only accepts the parameters, so we can create this \n",
    "# small wrapper function that calls our original chisq_likelihood\n",
    "# function with the necessary arguments.\n",
    "def loglike(theta):\n",
    "    args=(example_data_1D['x'],\n",
    "          example_data_1D['y'],\n",
    "          example_data_1D['error'],\n",
    "          my_model)\n",
    "    chisq = chisq_likelihood(theta,args)\n",
    "    return -1.*chisq #again we need to take the negative of the chi-square because we need a function to maximize\n",
    "\n",
    "#This can be a bit difficult to understand. We are using the same bounds\n",
    "#on our parameters as before here [a=(0,5),b=(-5,5),c=(0,10)].\n",
    "#However, nested sampling always reduces the problem to the \n",
    "#range (0,1) for every parameter. Then we always need to provide\n",
    "#a function prior_transform, which applies our prior to the model\n",
    "#and simultaneously tranforms from this unit space, to our actual\n",
    "#parameter space. This essentially means rescaling the parameter \n",
    "#from the range (0,1) to the range (-5,5).\n",
    "#\n",
    "#For example, we want a uniform prior on b, meaning bounds from -5 to 5.\n",
    "#The algorithm will only try values for b from 0 to 1. Suppose it tries\n",
    "#b=0.5. That's halfway between 0 and 1, which means it is halfway between\n",
    "#our bounds (-5,5), which is 0, not 0.5. So we need to find a way\n",
    "#to change 0.5-->0. We take the second parameter (b) and \n",
    "#multiply it by 10, giving us 5. Now we subtract 5,\n",
    "#so that a guess of b=0.5 transforms to b=0, as desired. \n",
    "#Test some other values of a,b,c to make sure you understand this step. \n",
    "#Would this same equation work for different bounds?...Nope.\n",
    "def prior_transform(parameters):\n",
    "    return np.array([5., 10., 10.]) * parameters + np.array([0., -5., 0.])\n",
    "\n",
    "# Run nested sampling.\n",
    "result_nest = nestle.sample(loglike, prior_transform, ndim=3,npoints=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see how we did\n",
    "a_nest, b_nest,c_nest = np.percentile(result_nest.samples, 50,axis=0)\n",
    "print('Chi:','a=',a_ml,'b=', b_ml,'c=',c_ml)\n",
    "print('MCMC:','a=',a_mcmc,'b=',b_mcmc,'c=',c_mcmc)\n",
    "print('Nested:','a=',a_nest,'b=',b_nest,'c=',c_nest)\n",
    "print('True:','a=',a_true,'b=', b_true,'c=',c_true)\n",
    "fig = corner.corner(result_nest.samples, \n",
    "                    labels=[\"$a$\", \"$b$\",\"$c$\"],\n",
    "                    quantiles=(0.16,0.50, 0.84),\n",
    "                    weights=result_nest.weights,#nested sampling provides weights for each sample based on bayesian evidence\n",
    "                    truths=[a_true, b_true,c_true])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see how Nested Sampling does for the \"bad\" case\n",
    "def loglike(theta):\n",
    "    args=(my_bad_data_1D['x'],my_bad_data_1D['y'],my_bad_data_1D['error'],my_bad_model)\n",
    "    chisq = chisq_likelihood(theta,args)\n",
    "    return -.5*chisq\n",
    "\n",
    "\n",
    "def prior_transform(x):\n",
    "    return np.array([5., 10., 10.]) * x + np.array([0., -5., 0.])\n",
    "\n",
    "# Run nested sampling.\n",
    "result_nest_bad = nestle.sample(loglike, prior_transform, 3,npoints=1000,maxcall=10000) #I know it'll have a difficult time converging, so I'm setting the maxcall to a finite number\n",
    "\n",
    "a_nest_bad, b_nest_bad,c_nest_bad = np.percentile(result_nest_bad.samples, 50,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see how we did\n",
    "print('Nested:','a=',a_nest_bad,'b=',b_nest_bad,'c=',c_nest_bad)\n",
    "print('True:','a=',a_true_bad,'b=', b_true_bad,'c=',c_true_bad)\n",
    "fig = corner.corner(result_nest_bad.samples, \n",
    "                    labels=[\"$a$\", \"$b$\",\"$c$\"],\n",
    "                    weights=result_nest_bad.weights,\n",
    "                    truths=[a_true_bad, b_true_bad,c_true_bad])\n",
    "plt.show()\n",
    "#The nested sampling perhaps did slightly better at mapping\n",
    "#The full parameter space, but we could always add more \n",
    "#walkers (or samples per walker) for the MCMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood with Assumed PDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's consider one other problem. Here instead of defining\n",
    "#A likelihood function and maximizing it, we want to \n",
    "#See how a theorist's model of a distribution does when\n",
    "#we compare it to experiment.\n",
    "#\n",
    "#Suppose we have a distribution of energies that we are\n",
    "#measuring. It has the following properties\n",
    "true_energy_mean = 10\n",
    "true_energy_std = 1\n",
    "N_energies = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(67)#Set the seed so everyone always gets the same data\n",
    "\n",
    "#This will pull N_energies (100) points from a normal distribution\n",
    "#with mean (loc) true_energy_mean, and standard deviation (scale)\n",
    "#true_energy_std.\n",
    "observed_energies=np.random.normal(loc=true_energy_mean,\n",
    "                                   scale=true_energy_std,\n",
    "                                   size=N_energies)\n",
    "plt.hist(observed_energies)\n",
    "plt.xlabel('Energy (GeV)')\n",
    "plt.ylabel('Number')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our theorist tells us that if we measure these energies,\n",
    "#we're going to see a perfect Gaussian. We set that up here\n",
    "def my_energy_model(energy_mean,energy_std,possible_energy):\n",
    "        return(scipy.stats.norm.pdf(possible_energy,\n",
    "                                    energy_mean,\n",
    "                                    energy_std))\n",
    "\n",
    "#Just see how this would actually compare before fitting\n",
    "possible_energies=np.arange(0,20,.1)\n",
    "model_energies=my_energy_model(10,1,possible_energies)\n",
    "plt.hist(observed_energies,density=True)\n",
    "plt.plot(possible_energies,model_energies)\n",
    "plt.xlabel('Energy (GeV)')\n",
    "plt.ylabel('Number')\n",
    "plt.show()\n",
    "#Not terrible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the all data likelihood function for all the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_data_likelihood_function(model_parameters):\n",
    "    model_mean,model_std = model_parameters\n",
    "    #Since our model IS a PDF, a single point on it\n",
    "    #would already be a likelihood. Therfore we can\n",
    "    #just take the value of the PDF at all of the \n",
    "    #observed energies, with the fitter trying a\n",
    "    #range of means and deviations, until we find\n",
    "    #the most likely distribution. This is the same\n",
    "    #as the commented code below, if it makes more\n",
    "    #sense to you one point at a time.\n",
    "    likelihoods = my_energy_model(model_mean,\n",
    "                                  model_std,\n",
    "                                  observed_energies)\n",
    "    #likelihoods = 1.\n",
    "    #for data_point in observed_energies:\n",
    "    #    single_datapoint_likelihood = my_energy_model(model_mean,\n",
    "    #                                                  model_std,\n",
    "    #                                                  data_point)\n",
    "    #    likelihoods *= single_datapoint_likelihood\n",
    "    return(np.sum(np.log(likelihoods)))#we take the sum of the log instead of the product of the original likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Does it work for a single point? Does the likelihood go down\n",
    "#If we move away from the true answer as it should? Yes.\n",
    "all_data_likelihood = all_data_likelihood_function( ( 10, 1) )\n",
    "print(all_data_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See above for an explanation of the prior transform. This\n",
    "#is the same as uniform bounds on our parameters of\n",
    "#mean=(0,20), std=(0,5)\n",
    "def prior_transform(x): \n",
    "    return np.array([20., 5.]) * x + np.array([0., 0.]) \n",
    "\n",
    "# Run nested sampling.\n",
    "energy_result_nest = nestle.sample(all_data_likelihood_function, \n",
    "                            prior_transform, ndim=2,npoints=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How was the theorist's prediction of the distribution? We\n",
    "#Could also compare this to other distributions. For sanity\n",
    "#We see that we recover the actual mean and standard deviations.\n",
    "mean_nest, std_nest = np.percentile(energy_result_nest.samples, 50,axis=0)\n",
    "print('Nested:','a=',mean_nest,'b=',std_nest)\n",
    "print('True:','a=',true_energy_mean,'b=', true_energy_std)\n",
    "fig = corner.corner(energy_result_nest.samples, \n",
    "                    labels=[\"$\\mu$\", \"$\\sigma$\"],\n",
    "                    weights=energy_result_nest.weights,\n",
    "                    truths=[true_energy_mean, true_energy_std])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
